{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import re \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/yangyuting/Library/Mobile Documents/com~apple~CloudDocs/01PhD/02ISU/04Classes/2022Fall/COM S 579X NLP/Project/wikidata-qrank.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieve entity names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Decode into bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[:1000]\n",
    "list1 = df1.Entity.to_list()\n",
    "Name = []\n",
    "for i, entity in enumerate(list1):\n",
    "    print('Entity', i)\n",
    "    response = urllib.request.urlopen('https://www.wikidata.org/w/api.php?action=wbgetentities&props=labels&ids='+entity+'&languages=en&format=json')\n",
    "    value = response.read().decode()\n",
    "    try:\n",
    "        value = value.split('\"value\":\"')[1].split('\"}}}}')[0]\n",
    "        Name.append(value)\n",
    "    except:\n",
    "        Name.append('NA')\n",
    "df1.insert(1, column = 'Name', value = Name)\n",
    "del i, entity, Name, list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Name</th>\n",
       "      <th>QRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Q110918355</td>\n",
       "      <td>NA</td>\n",
       "      <td>34741612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entity Name     QRank\n",
       "18  Q110918355   NA  34741612"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.query('Name == \"NA\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"entities\":{\"Q1107\":{\"type\":\"item\",\"id\":\"Q1107\",\"labels\":{\"en\":{\"language\":\"en\",\"value\":\"anime\"}}}},\"success\":1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = urllib.request.urlopen('https://www.wikidata.org/w/api.php?action=wbgetentities&props=labels&ids=Q1107&languages=en&format=json')\n",
    "value = response.read().decode()\n",
    "print(value)\n",
    "# value = value.split('\"value\":\"')[1].split('\"}}}}')[0]\n",
    "# Name.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q1107'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = df1.Entity.to_list()\n",
    "list1[999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Decode into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_NEs(input_csv_ranking_file_path: str, output_csv_file_path: str, top_N: int = 10):\n",
    "    df = pd.read_csv(input_csv_ranking_file_path)\n",
    "    df2 = df[:top_N]\n",
    "    list2 = df2.Entity.to_list()\n",
    "    Name = []\n",
    "    for i, entity in enumerate(list2):\n",
    "        print('Entity', i)\n",
    "        response = urllib.request.urlopen('https://www.wikidata.org/w/api.php?action=wbgetentities&props=labels&ids='+entity+'&languages=en&format=json')\n",
    "        value = response.read().decode('utf-8')\n",
    "        value = json.loads(value)\n",
    "        print(type(entity))\n",
    "        try:\n",
    "            value = value['entities'][entity]['labels']['en']['value']\n",
    "            # value = value['entities'][f'{entity}]['labels']['en']['value']\n",
    "            Name.append(value)\n",
    "        except:\n",
    "            Name.append('NA')\n",
    "    df2.insert(1, column = 'Name', value = Name)\n",
    "    df2.to_csv(output_csv_file_path)\n",
    "    del i, entity, Name, list2\n",
    "\n",
    "top_N_NEs(\"/Users/yangyuting/Library/Mobile Documents/com~apple~CloudDocs/01PhD/02ISU/04Classes/2022Fall/COM S 579X NLP/Project/wikidata-qrank.csv\",\n",
    "          \"/Users/yangyuting/Library/Mobile Documents/com~apple~CloudDocs/01PhD/02ISU/04Classes/2022Fall/COM S 579X NLP/Project/wikidata-qrank_top10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entities': {'Q317521': {'type': 'item', 'id': 'Q317521', 'labels': {'en': {'language': 'en', 'value': 'Elon Musk'}}}}, 'success': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Elon Musk'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Q317521\" \"Q1107\"\n",
    "response = urllib.request.urlopen('https://www.wikidata.org/w/api.php?action=wbgetentities&props=labels&ids=Q317521&languages=en&format=json')\n",
    "value = response.read()\n",
    "value = json.loads(value.decode('utf-8'))\n",
    "print(value)\n",
    "value['entities']['Q317521']['labels']['en']['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2nd two weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Goal 0: Expand top_n_NEs above to return the top-N NEs as a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_NEs(input_csv_ranking_file_path: str, top_N: int = 10):\n",
    "    df = pd.read_csv(input_csv_ranking_file_path)\n",
    "    df2 = df[:top_N]\n",
    "    list2 = df2.Entity.to_list()\n",
    "    Name = []\n",
    "    for i, entity in enumerate(list2):\n",
    "        print('Entity', i)\n",
    "        response = urllib.request.urlopen('https://www.wikidata.org/w/api.php?action=wbgetentities&props=labels&ids='+entity+'&languages=en&format=json')\n",
    "        value = response.read().decode('utf-8')\n",
    "        value = json.loads(value)\n",
    "        print(type(entity))\n",
    "        try:\n",
    "            value = value['entities'][entity]['labels']['en']['value']\n",
    "            # value = value['entities'][f'{entity}]['labels']['en']['value']\n",
    "            Name.append(value)\n",
    "        except:\n",
    "            Name.append('NA')\n",
    "    return Name\n",
    "    del i, entity, Name, list2\n",
    "\n",
    "list1 = top_N_NEs(\"/Users/yangyuting/Library/Mobile Documents/com~apple~CloudDocs/01PhD/02ISU/04Classes/2022Fall/COM S 579X NLP/Project/wikidata-qrank.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Goal 1: Understand how to evaluate an NER model/method based on the papers you read in Goal 2 for the first two weeks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOTA performance: F1 score, precision, and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal 2: Develop a Python function that takes a sentence as input and returns a NER tag sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(in an appropriate tagging schemes, such as BIO), using simple string match pseudocode below. One thing you need to think is how to match overlapping NEs, e.g., “iowa” vs. “iowa state university”. One easy heurstics is to always match the longest one.\n",
    "\n",
    "[Named Entity Recognition Tagging](https://cs230.stanford.edu/blog/namedentity/)  \n",
    "[NER model evaluation](https://towardsdatascience.com/entity-level-evaluation-for-ner-task-c21fb3a8edf)  \n",
    "[Spacy custom tokenizer to include only hyphen words as tokens using Infix regex](https://stackoverflow.com/questions/51012476/spacy-custom-tokenizer-to-include-only-hyphen-words-as-tokens-using-infix-regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. CONLL2003 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll = urllib.request.urlopen(\"https://datasets-server.huggingface.co/splits?dataset=conll2003\")\n",
    "dic = conll.read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_nertag = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_ner_tags(tags):\n",
    "    for id, i in enumerate(tags):\n",
    "        # print(tags)\n",
    "        for index, value in conll_nertag.items():\n",
    "            if i == value:\n",
    "                tags[id] = index.split('-')[0]\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll = pd.DataFrame([\n",
    "    [[ \"China\", \"controlled\", \"most\", \"of\", \"the\", \"match\", \"and\", \"saw\", \"several\", \"chances\", \"missed\", \"until\", \"the\", \"78th\", \"minute\", \"when\", \"Uzbek\", \"striker\", \"Igor\", \"Shkvyrin\", \"took\", \"advantage\", \"of\", \"a\", \"misdirected\", \"defensive\", \"header\", \"to\", \"lob\", \"the\", \"ball\", \"over\", \"the\", \"advancing\", \"Chinese\", \"keeper\", \"and\", \"into\", \"an\", \"empty\", \"net\", \".\" ], [ 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0 ]],\n",
    "    [[ \"Oleg\", \"Shatskiku\", \"made\", \"sure\", \"of\", \"the\", \"win\", \"in\", \"injury\", \"time\", \",\", \"hitting\", \"an\", \"unstoppable\", \"left\", \"foot\", \"shot\", \"from\", \"just\", \"outside\", \"the\", \"area\", \".\" ], [ 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]]\n",
    "],\n",
    "    columns = ('tokens', 'ner_tags')\n",
    ")\n",
    "conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net.', 'Oleg Shatskiku made sure of the win in injury time, hitting an unstoppable left foot shot from just outside the area.']\n",
      "[['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "conll_sen = [TreebankWordDetokenizer().detokenize(i) for i in conll.tokens.to_list()]\n",
    "print(conll_sen)\n",
    "\n",
    "conll_ner = [change_ner_tags(a) for a in conll.ner_tags.to_list()]\n",
    "print(conll_ner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Tokenization setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "sen = custom_tokenizer(\"I joined the Ph.D. program of Ivy College of Business at Iowa State University in Ames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'Yuting', 'Yang', '.', 'I', 'joined', 'the', 'Ph.D.', 'program', 'of', 'Ivy', 'College', 'of', 'Business', 'at', 'Iowa', 'State', 'University', 'in', 'Ames', 'on', '2022-08-15', '.', 'People', 'are', 'open-minded', 'here', '.', 'The', 'water', 'cost', '$', '2.00', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sen = \"I'm Yuting Yang. I joined the Ph.D. program of Ivy College of Business at Iowa State University in Ames on 2022-08-15. People are open-minded here. The water costs $2.00.\"\n",
    "print(word_tokenize(sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. NER tagging function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "sen = \"I joined the Ph.D. program of Ivy College of Business at Iowa State University in Ames.\"\n",
    "NE_list = ['Iowa State University', 'College of Business','Ivy College of Business', 'Iowa', 'Ivy','Ames']\n",
    "\n",
    "def brutal_force_ENR(sentence:str, NE_list: list[str], scheme='BIO'):\n",
    "    sen_list = word_tokenize(sentence)\n",
    "    # print(sen_list)\n",
    "    return_list = ['O']*len(sen_list)\n",
    "    # print('\\nThe list to be revised: \\n',return_list, '\\n')\n",
    "    \n",
    "    pos_list = []\n",
    "    for ne in NE_list:\n",
    "        position_start = sentence.find(ne)\n",
    "        position_end = position_start + len(ne) - 1\n",
    "        l = len(ne.split())\n",
    "        print(ne, position_start, position_end, l)\n",
    "        pos_list.append([l, position_start])\n",
    "    # print('\\n The number of tokens in the entity, the starting position of the entity:\\n', pos_list)\n",
    "    \n",
    "    str_len = 0\n",
    "    id_list = []\n",
    "    for id, token in enumerate(sen_list):\n",
    "        str_len += len(token) + 1\n",
    "        for l, pos in pos_list:\n",
    "            if pos == 0:\n",
    "                id_list.append([0, l])\n",
    "            elif str_len == pos:\n",
    "                id_list.append([id+1, l])\n",
    "    # print('\\nThe position in the return_list, the number of tokens to be revised\\n',id_list)\n",
    "    \n",
    "    for id, length in id_list:\n",
    "        if return_list[id] == 'I':\n",
    "            pass\n",
    "        else:\n",
    "            return_list[id] = 'B'\n",
    "            while length > 1:\n",
    "                return_list[id+1] = 'I'\n",
    "                length -= 1\n",
    "                id += 1\n",
    "    # print('\\n Output: \\n', sen_list, '\\n', return_list)\n",
    "    print('\\n Tokenize: \\n', sen_list)\n",
    "    return return_list\n",
    "# brutal_force_ENR(sen, NE_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Performance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from seqeval.metrics import accuracy_score\n",
    "# from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "def a_judge_function(predicted_tag_seq: list[list], ground_truth_tag_seq: list[list]):\n",
    "    return classification_report([ground_truth_tag_seq], [predicted_tag_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report([['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O']], [['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal 3 Test brutal_force_NER on the CoNLL2003 and collect its performance quantities on CoNLL2003.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_CONLL(sentences: list[str], ground_truth_tag_sequences: list[str], NE_list: list[str], scheme=\"BIO\"):\n",
    "    for (sentence, ground_truth_tag_seq) in zip(sentences, ground_truth_tag_sequences):\n",
    "        print(sentence)\n",
    "        tag_seq = brutal_force_ENR(sentence, NE_list, scheme = scheme)\n",
    "        print('\\nGround truth\\n', ground_truth_tag_seq, '\\nBrutal force tagging\\n', tag_seq)\n",
    "        performance_matrix = a_judge_function(ground_truth_tag_seq, tag_seq)\n",
    "    # return len(sentences)\n",
    "    return performance_matrix, len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net.\n",
      "China 0 4 1\n",
      "Uzbek 93 97 1\n",
      "Igor Shkvyrin 107 119 2\n",
      "Chinese 205 211 1\n",
      "\n",
      " Tokenize: \n",
      " ['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
      "\n",
      "Ground truth\n",
      " ['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] \n",
      "Brutal force tagging\n",
      " ['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('              precision    recall  f1-score   support\\n\\n           _       1.00      1.00      1.00         4\\n\\n   micro avg       1.00      1.00      1.00         4\\n   macro avg       1.00      1.00      1.00         4\\nweighted avg       1.00      1.00      1.00         4\\n',\n",
       " 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# sen = \"I joined the Ph.D. program of Ivy College of Business at Iowa State University in Ames.\"\n",
    "# NE_list = ['Iowa State University', 'Ivy College of Business', 'Iowa', 'Ivy', 'Ames']\n",
    "# true_list = ['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B','O']\n",
    "NE_list = ['China', 'Uzbek', 'Igor Shkvyrin','Chinese']\n",
    "\n",
    "test_on_CONLL([conll_sen[0]], [conll_ner[0]], NE_list) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c528eded6f1c51023d5da767c54de3181c50a5f19d2b16c3dc73092ab341481"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
